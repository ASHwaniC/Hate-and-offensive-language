{"cells":[{"metadata":{"_uuid":"4dad047577e3a2bd257f80d280f9bbbe09a302c6"},"cell_type":"markdown","source":"# Replication for results in Davidson et al. 2017. \"Automated Hate Speech Detection and the Problem of Offensive Language\""},{"metadata":{"trusted":true,"_uuid":"2c13f69606878b8f23cd404d040093c264f045b3","collapsed":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport pickle\nimport sys\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport nltk\nfrom nltk.stem.porter import *\nimport string\nimport re\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer as VS\nfrom textstat.textstat import *\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.metrics import classification_report\nfrom sklearn.svm import LinearSVC\nimport matplotlib.pyplot as plt\nimport seaborn\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4a4b782471b45949bebe0dd60bada3935c3d6716"},"cell_type":"markdown","source":"## Loading the data"},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"fc621965ea2d51cc865f373300baaf88a0d10261"},"cell_type":"code","source":"df = pd.read_csv(\"../input/hate-data/labeled_data.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"12e485cd54bb648ef14ce4cea255784c0a3344d6","collapsed":true},"cell_type":"code","source":"df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bc73c4178031377d65703cae0eeebe5d7cb0a4b3","collapsed":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a95d2f444b504bf32c26bdd5be254375c52cf2bd","collapsed":true},"cell_type":"code","source":"df.columns","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"919a7911d73c538f67e106f64ecc00075df6f83e"},"cell_type":"markdown","source":"### Columns key:\ncount = number of CrowdFlower users who coded each tweet (min is 3, sometimes more users coded a tweet when judgments were determined to be unreliable by CF).\n\n\nhate_speech = number of CF users who judged the tweet to be hate speech.\n\n\noffensive_language = number of CF users who judged the tweet to be offensive.\n\n\nneither = number of CF users who judged the tweet to be neither offensive nor non-offensive.\n\n\nclass = class label for majority of CF users.\n\n    0 - hate speech\n    1 - offensive  language\n    2 - neither\n\ntweet = raw tweet text\n"},{"metadata":{"trusted":true,"_uuid":"938ffe59647ef0c6461efef5804edafe72b79c44","collapsed":true},"cell_type":"code","source":"df['class'].hist()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"69a251e82a5ee2dac017de1974f665e8bf13b2b5"},"cell_type":"markdown","source":"This histogram shows the imbalanced nature of the task - most tweets containing \"hate\" words as defined by Hatebase were \nonly considered to be offensive by the CF coders. More tweets were considered to be neither hate speech nor offensive language than were considered hate speech."},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"cb11d4e70f499b101c0208836c9a8f38fbd5a201"},"cell_type":"code","source":"tweets=df.tweet","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"7d2185d42a72c283fc9e8be6504f93001c40f611"},"cell_type":"markdown","source":"## Feature generation"},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"377eac9a7eb08078b6ed6c4457115778b93823af"},"cell_type":"code","source":"stopwords = nltk.corpus.stopwords.words(\"english\")\n\nother_exclusions = [\"#ff\", \"ff\", \"rt\"]\nstopwords.extend(other_exclusions)\n\nstemmer = PorterStemmer()\n\n\ndef preprocess(text_string):\n    \"\"\"\n    Accepts a text string and replaces:\n    1) urls with URLHERE\n    2) lots of whitespace with one instance\n    3) mentions with MENTIONHERE\n\n    This allows us to get standardized counts of urls and mentions\n    Without caring about specific people mentioned\n    \"\"\"\n    space_pattern = '\\s+'\n    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n    mention_regex = '@[\\w\\-]+'\n    parsed_text = re.sub(space_pattern, ' ', text_string)\n    parsed_text = re.sub(giant_url_regex, '', parsed_text)\n    parsed_text = re.sub(mention_regex, '', parsed_text)\n    return parsed_text\n\ndef tokenize(tweet):\n    \"\"\"Removes punctuation & excess whitespace, sets to lowercase,\n    and stems tweets. Returns a list of stemmed tokens.\"\"\"\n    tweet = \" \".join(re.split(\"[^a-zA-Z]*\", tweet.lower())).strip()\n    tokens = [stemmer.stem(t) for t in tweet.split()]\n    return tokens\n\ndef basic_tokenize(tweet):\n    \"\"\"Same as tokenize but without the stemming\"\"\"\n    tweet = \" \".join(re.split(\"[^a-zA-Z]*\", tweet.lower())).strip()\n    return tweet.split()\n\nvectorizer = TfidfVectorizer(\n    tokenizer=tokenize,\n    preprocessor=preprocess,\n    ngram_range=(1, 3),\n    stop_words=stopwords,\n    use_idf=True,\n    smooth_idf=False,\n    norm=None,\n    decode_error='replace',\n    max_features=1000,\n    min_df=5,\n    max_df=0.75\n    )","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"06e839d1a610cf020e25fab0cdb1833082da8cde"},"cell_type":"code","source":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aff2612f981c0879f625b5125afaba933b371499","collapsed":true},"cell_type":"code","source":"#Construct tfidf matrix and get relevant scores\ntfidf = vectorizer.fit_transform(tweets).toarray()\nvocab = {v:i for i, v in enumerate(vectorizer.get_feature_names())}\nidf_vals = vectorizer.idf_\nidf_dict = {i:idf_vals[i] for i in vocab.values()} #keys are indices; values are IDF scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"961be5b1ae7a55f3b4cd55d772076a50da6a5bbd","collapsed":true},"cell_type":"code","source":"tfidf.shape","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"74e6c9b1050f31077f9024ae3251f2b6f07fc5e5"},"cell_type":"markdown","source":"#Get POS tags for tweets and save as a string\ntweet_tags = []\nfor t in tweets:\n    tokens = basic_tokenize(preprocess(t))\n    tags = nltk.pos_tag(tokens)\n    tag_list = [x[1] for x in tags]\n    tag_str = \" \".join(tag_list)\n    tweet_tags.append(tag_str)"},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"ac75de19b9d1e7fc0f216a950541a020e0d8d941"},"cell_type":"markdown","source":"#We can use the TFIDF vectorizer to get a token matrix for the POS tags\npos_vectorizer = TfidfVectorizer(\n    tokenizer=None,\n    lowercase=False,\n    preprocessor=None,\n    ngram_range=(1, 3),\n    stop_words=None,\n    use_idf=True, # isme True lagake dekhna hai\n    smooth_idf=False,\n    norm=None,\n    decode_error='replace',\n    max_features=5000,\n    min_df=5,\n    max_df=0.75,\n    )"},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"0c4366798098c27cb3ac860d2374fbbfa4095baf"},"cell_type":"markdown","source":"#Construct POS TF matrix and get vocab dict\npos = pos_vectorizer.fit_transform(pd.Series(tweet_tags)).toarray()\npos_vocab = {v:i for i, v in enumerate(pos_vectorizer.get_feature_names())}"},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"93c4120bbcbefaf0420d88c588c4975461af06bb"},"cell_type":"markdown","source":"#Now get other features\nsentiment_analyzer = VS()\n\ndef count_twitter_objs(text_string):\n    \"\"\"\n    Accepts a text string and replaces:\n    1) urls with URLHERE\n    2) lots of whitespace with one instance\n    3) mentions with MENTIONHERE\n    4) hashtags with HASHTAGHERE\n\n    This allows us to get standardized counts of urls and mentions\n    Without caring about specific people mentioned.\n    \n    Returns counts of urls, mentions, and hashtags.\n    \"\"\"\n    space_pattern = '\\s+'\n    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n    mention_regex = '@[\\w\\-]+'\n    hashtag_regex = '#[\\w\\-]+'\n    parsed_text = re.sub(space_pattern, ' ', text_string)\n    parsed_text = re.sub(giant_url_regex, 'URLHERE', parsed_text)\n    parsed_text = re.sub(mention_regex, 'MENTIONHERE', parsed_text)\n    parsed_text = re.sub(hashtag_regex, 'HASHTAGHERE', parsed_text)\n    return(parsed_text.count('URLHERE'),parsed_text.count('MENTIONHERE'),parsed_text.count('HASHTAGHERE'))\n\ndef other_features(tweet):\n    \"\"\"This function takes a string and returns a list of features.\n    These include Sentiment scores, Text and Readability scores,\n    as well as Twitter specific features\"\"\"\n    sentiment = sentiment_analyzer.polarity_scores(tweet)\n    \n    words = preprocess(tweet) #Get text only\n    \n    syllables = textstat.syllable_count(words)\n    num_chars = sum(len(w) for w in words)\n    num_chars_total = len(tweet)\n    num_terms = len(tweet.split())\n    num_words = len(words.split())\n    avg_syl = round(float((syllables+0.001))/float(num_words+0.001),4)\n    num_unique_terms = len(set(words.split()))\n    \n    ###Modified FK grade, where avg words per sentence is just num words/1\n    FKRA = round(float(0.39 * float(num_words)/1.0) + float(11.8 * avg_syl) - 15.59,1)\n    ##Modified FRE score, where sentence fixed to 1\n    FRE = round(206.835 - 1.015*(float(num_words)/1.0) - (84.6*float(avg_syl)),2)\n    \n    twitter_objs = count_twitter_objs(tweet)\n    retweet = 0\n    if \"rt\" in words:\n        retweet = 1\n    features = [FKRA, FRE,syllables, avg_syl, num_chars, num_chars_total, num_terms, num_words,\n                num_unique_terms, sentiment['neg'], sentiment['pos'], sentiment['neu'], sentiment['compound'],\n                twitter_objs[2], twitter_objs[1],\n                twitter_objs[0], retweet]\n    #features = pandas.DataFrame(features)\n    return features\n\ndef get_feature_array(tweets):\n    feats=[]\n    for t in tweets:\n        feats.append(other_features(t))\n    return np.array(feats)"},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"c5772c5693659dc652ffb4c5c64c82abf4c8c829"},"cell_type":"markdown","source":"other_features_names = [\"FKRA\", \"FRE\",\"num_syllables\", \"avg_syl_per_word\", \"num_chars\", \"num_chars_total\", \\\n                        \"num_terms\", \"num_words\", \"num_unique_words\", \"vader neg\",\"vader pos\",\"vader neu\", \\\n                        \"vader compound\", \"num_hashtags\", \"num_mentions\", \"num_urls\", \"is_retweet\"]"},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"24ebde0c74b66d4045ade4c92ee3d5bae84ee15d"},"cell_type":"markdown","source":"feats = get_feature_array(tweets)"},{"metadata":{"trusted":true,"_uuid":"67b63689c8470d02725b6bf172ba626f0a97111f","collapsed":true},"cell_type":"code","source":"tweet_fast = []\nfor t in tweets:\n    tokens = basic_tokenize(preprocess(t))\n    ll = []\n    for l in tokens:\n        if l not in stopwords:\n            ll.append(l)\n    tweet_fast.append(ll)\n#tweet_fast","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7116b44c17ce9413b18843cd8c408f2da0a440cf","collapsed":true},"cell_type":"code","source":"from gensim.models import FastText as ft\nmodel1=ft.load_fasttext_format(\"../input/fasttext-pretrained-word-vectors-english/wiki.en.bin\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"5f4f84d5c2f30f366f06b5d26f5d5939b0c01101"},"cell_type":"code","source":"def makeFeatureVec(words, model, num_features):\n    # Function to average all of the word vectors in a given\n    # paragraph\n    #\n    # Pre-initialize an empty numpy array (for speed)\n    featureVec = np.zeros((num_features,),dtype=\"float32\")\n    #\n    nwords = 0\n    # \n    # Index2word is a list that contains the names of the words in \n    # the model's vocabulary. Convert it to a set, for speed \n    #index2word_set = set(model.wv.index2word) - loadline\n    #\n    # Loop over each word in the review and, if it is in the model's\n    # vocaublary, add its feature vector to the total\n    for word in words:\n        nwords = nwords + 1\n        featureVec = np.add(featureVec,model[word])\n    # \n    # Divide the result by the number of words to get the average\n    featureVec = np.divide(featureVec,nwords)\n    return featureVec\n\n\ndef getAvgFeatureVecs(reviews, model, num_features):\n    # Given a set of reviews (each one a list of words), calculate \n    # the average feature vector for each one and return a 2D numpy array \n    # \n    # Initialize a counter\n    counter = 0\n    # \n    # Preallocate a 2D numpy array, for speed\n    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n    # \n    # Loop through the reviews\n    for review in reviews:\n       #\n       # Print a status message every 1000th review\n        if counter%1000 == 0:\n            print(\"Review %d of %d\" % (counter, len(reviews)))\n       # \n       # Call the function (defined above) that makes average feature vectors\n        reviewFeatureVecs[counter] = makeFeatureVec(review, model, \\\n           num_features)\n       #\n       # Increment the counter\n        counter = counter + 1\n    return reviewFeatureVecs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5591d35105628e232ac128f20a4de0e1f7988b63","collapsed":true},"cell_type":"code","source":"trainDataVecs = getAvgFeatureVecs( tweet_fast, model1, 300 )","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"2823f6828461f84e671b46f9ea68b5700b9e2e7a"},"cell_type":"code","source":"#Now join them all up\nM = np.concatenate([tfidf,trainDataVecs],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ee702002d5828eda5605aa6a37b515e82b360dd8","collapsed":true},"cell_type":"code","source":"M.shape","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"701bd8bea3684254ed643ebf0b161ca1a9438220"},"cell_type":"markdown","source":"#Finally get a list of variable names\nvariables = ['']*len(vocab)\nfor k,v in vocab.items():\n    variables[v] = k\n\npos_variables = ['']*len(pos_vocab)\nfor k,v in pos_vocab.items():\n    pos_variables[v] = k\n\nfeature_names = variables+pos_variables+other_features_names"},{"metadata":{"_uuid":"ec7a1a7397da048c1c2300882639a04fc9aab8b0"},"cell_type":"markdown","source":"# Running the model\n\nThe best model was selected using a GridSearch with 5-fold CV."},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"169c7befe7450ebd149f29112402d0a0f25a2d15"},"cell_type":"code","source":"X = pd.DataFrame(M)\ny = df['class'].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1a99e73af4819581a31d39c57c31fd9c0b5e4775","collapsed":true},"cell_type":"code","source":"X=X.fillna(0)\nX.shape","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"a73251094ae234c9da8b0b876b3051f14496a2a0"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"03849577d1ee15baf1dda3ea5baa8a7084f0a8ec","collapsed":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.1)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"acc07087020dfa4938d4e10cfeaba41dac2af2a4"},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold, GridSearchCV\nfrom sklearn.pipeline import Pipeline","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"ef91a9e3a85e8364a79102e06054563d3a53aeef"},"cell_type":"code","source":"w = {0:0.950, 1:0.005, 2:0.045}","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"9c7dd5a1980730b1dbb97c04fd1f32105c78143e"},"cell_type":"code","source":"pipe = Pipeline(\n        [('select', SelectFromModel(LogisticRegression(class_weight=w,\n                                                  penalty=\"l1\"))),\n        ('model', LogisticRegression(class_weight=w,penalty='l2'))])","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"30e097bb4d2d32e738c625d21f5a587453840a86"},"cell_type":"code","source":"param_grid = [{}] # Optionally add parameters here","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"92121bb3d337ecd4b3568b582dd1e57a91209c52"},"cell_type":"code","source":"grid_search = GridSearchCV(pipe, \n                           param_grid,\n                           cv=StratifiedKFold(n_splits=5, \n                                              random_state=42).split(X_train, y_train), \n                           verbose=2)","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true,"_uuid":"7832b9b69c25a697b19a10d6fd87c095b3d66d09","collapsed":true},"cell_type":"code","source":"model = grid_search.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"2db1f92d65a2dc762f72b0e96214b3fcbc28c0d8"},"cell_type":"code","source":"y_preds = model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"878ac4456cbc50c9623899aef2687be3d6804fed"},"cell_type":"markdown","source":"## Evaluating the results"},{"metadata":{"collapsed":true,"trusted":true,"_uuid":"2eb7d081e426dba3f68854a39963211426b54751"},"cell_type":"code","source":"report = classification_report( y_test, y_preds )","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c051992c82a505a5816966b10f6af4e78c7b5cec"},"cell_type":"markdown","source":"***Note: Results in paper are from best model retrained on the entire dataset (see the other notebook). Here the results are reported after using cross-validation and only for the held-out set.***"},{"metadata":{"trusted":true,"_uuid":"d438141b3adac87e59860a2b9486c428c22fdba5","collapsed":true},"cell_type":"code","source":"print(report)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1a2e2eb79f53525752acd58836a2affd745c74f5","collapsed":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\nconfusion_matrix = confusion_matrix(y_test,y_preds)\nmatrix_proportions = np.zeros((3,3))\nfor i in range(0,3):\n    matrix_proportions[i,:] = confusion_matrix[i,:]/float(confusion_matrix[i,:].sum())\nnames=['Hate','Offensive','Neither']\nconfusion_df = pd.DataFrame(matrix_proportions, index=names,columns=names)\nplt.figure(figsize=(5,5))\nseaborn.heatmap(confusion_df,annot=True,annot_kws={\"size\": 12},cmap='gist_gray_r',cbar=False, square=True,fmt='.2f')\nplt.ylabel(r'True categories',fontsize=14)\nplt.xlabel(r'Predicted categories',fontsize=14)\nplt.tick_params(labelsize=12)\n\n#Uncomment line below if you want to save the output\n#plt.savefig('confusion.pdf')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"60a4898351a663bee0aefe97f9c71ea0e01087ed","collapsed":true},"cell_type":"code","source":"#True distribution\ny.hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ff9b718fb55798ee027d4430a5b247d95c8e9d1a","collapsed":true},"cell_type":"code","source":"pd.Series(y_preds).hist()","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"a9f02bb7e4808f9659b646fe14f1d5dee29b9507"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"5a064d7bd23cdc420e933df4403ee4a973cc1a16"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"collapsed":true,"trusted":false,"_uuid":"a96689ba3739d24880567abc0203cc6bd90ce0c5"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}